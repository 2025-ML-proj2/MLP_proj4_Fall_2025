{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================================================================\n# ENSEMBLED MODEL: LightGBM Classifier + XGBoost Fallback + GBM Regressor\n# =============================================================================\n\nimport os\nimport logging\nfrom pathlib import Path\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n# --- Kaggle Evaluation API Interface (with Mock for local) ---\ntry:\n    import kaggle_evaluation.default_inference_server as inference_server_mod\n\n    DefaultInferenceServer = inference_server_mod.DefaultInferenceServer\nexcept ImportError:\n    class MockInferenceServer:\n        def __init__(self, predict_fn):\n            self.predict_fn = predict_fn\n\n        def serve(self):\n            logging.info(\"Mock Inference Server Running...\")\n\n        def run_local_gateway(self, path):\n            logging.info(f\"Mock Local Gateway Running with path: {path}\")\n\n    DefaultInferenceServer = MockInferenceServer\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nDATA_PATH = Path(\"/kaggle/input/hull-tactical-market-prediction/\")\n\nprint(\"=\" * 80)\nprint(\"ENSEMBLED HULL TACTICAL MODEL\")\nprint(\"=\" * 80)\n\n# =============================================================================\n# ENSEMBLE WEIGHTS  (← 여기만 바꿔서 가중치 조절하면 됨)\n# =============================================================================\n# LightGBM Classifier 기반 모델\nWEIGHT_LGBM = 0.7\n# Simple Alpha + XGBoost Fallback 모델\nWEIGHT_XGB = 0.3\n# GradientBoostingRegressor + post_process_signal 모델\nWEIGHT_GB = 0.0\n\n# -----------------------------------------------------------------------------\n# HELPER: Ensemble Allocation\n# -----------------------------------------------------------------------------\ndef ensemble_allocation(alloc_lgb, alloc_xgb, alloc_gb):\n    weights = np.array([WEIGHT_LGBM, WEIGHT_XGB, WEIGHT_GB], dtype=float)\n    allocs = np.array([alloc_lgb, alloc_xgb, alloc_gb], dtype=float)\n    wsum = weights.sum()\n    if wsum <= 0:\n        # 비정상적인 경우, LightGBM 결과만 사용\n        return float(np.clip(alloc_lgb, 0.0, 2.0))\n    final = float(np.dot(weights, allocs) / wsum)\n    return float(np.clip(final, 0.0, 2.0))\n\n\n# =============================================================================\n# MODEL 1: LightGBM Classifier + Feature Engineering (원본 구조 유지, 이름만 변경)\n# =============================================================================\n\nMODEL_LGB = None\nTRAIN_COLS_LGB = None\nTRAIN_MEANS_LGB = None\n\ndef get_features(df: pl.DataFrame) -> List[str]:\n    \"\"\"원본 코드의 get_features (현재는 직접 사용하지 않지만 유지).\"\"\"\n    EXCLUDE_COLS = [\n        \"date_id\",\n        \"forward_returns\",\n        \"risk_free_rate\",\n        \"market_forward_excess_returns\",\n        \"is_scored\",\n        \"lagged_forward_returns\",\n        \"lagged_risk_free_rate\",\n        \"lagged_market_forward_excess_returns\",\n    ]\n    feature_cols = [col for col in df.columns if col not in EXCLUDE_COLS]\n    return feature_cols\n\ndef preprocess_data_lgb(df: pl.DataFrame, is_training: bool = False) -> pd.DataFrame:\n    \"\"\"\n    원본 LightGBM 모델의 Feature Engineering 로직.\n    \"\"\"\n    if \"date_id\" in df.columns:\n        df = df.with_columns((pl.col(\"date_id\") % 5).alias(\"day_of_cycle\"))\n\n    ROLLING_WINDOWS = [5, 10, 20]\n    BASE_FEATURES = [\"M1\", \"E1\", \"V1\", \"S1\", \"T1\", \"P1\", \"D1\"]\n\n    expressions = []\n    LAG_WINDOWS = [1, 5]\n    for lag in LAG_WINDOWS:\n        for col in BASE_FEATURES:\n            if col in df.columns:\n                expressions.append(pl.col(col).shift(lag).alias(f\"{col}_lag_{lag}\"))\n\n    for window in ROLLING_WINDOWS:\n        for col in BASE_FEATURES:\n            if col in df.columns:\n                expressions.append(\n                    pl.col(col)\n                    .rolling_mean(window_size=window, min_samples=1)\n                    .alias(f\"{col}_roll_mean_{window}\")\n                )\n                expressions.append(\n                    pl.col(col)\n                    .rolling_std(window_size=window, min_samples=1)\n                    .alias(f\"{col}_roll_std_{window}\")\n                )\n\n    if expressions:\n        df = df.with_columns(expressions)\n\n    pdf = df.to_pandas()\n\n    EMA_WINDOWS = [10, 30, 60]\n    for window in EMA_WINDOWS:\n        for col in BASE_FEATURES:\n            if col in pdf.columns:\n                pdf[f\"{col}_ema_{window}\"] = pdf[col].ewm(\n                    span=window, adjust=False\n                ).mean()\n\n    FEATURE_PAIRS = [\n        (\"M1\", \"M2\"),\n        (\"E1\", \"E2\"),\n        (\"V1\", \"V2\"),\n        (\"S1\", \"S2\"),\n        (\"T1\", \"T2\"),\n        (\"P1\", \"P2\"),\n        (\"D1\", \"D2\"),\n    ]\n\n    for col1, col2 in FEATURE_PAIRS:\n        if col1 in pdf.columns and col2 in pdf.columns:\n            pdf[f\"{col1}_div_{col2}\"] = pdf[col1] / (\n                pdf[col2].replace(0, 1e-6) + 1e-6\n            )\n            pdf[f\"{col1}_minus_{col2}\"] = pdf[col1] - pdf[col2]\n\n    EXCLUDE_FINAL_COLS = [\n        \"date_id\",\n        \"forward_returns\",\n        \"risk_free_rate\",\n        \"market_forward_excess_returns\",\n        \"is_scored\",\n        \"lagged_forward_returns\",\n        \"lagged_risk_free_rate\",\n        \"lagged_market_forward_excess_returns\",\n    ]\n    final_cols = [col for col in pdf.columns if col not in EXCLUDE_FINAL_COLS]\n    return pdf[final_cols]\n\ndef train_model_lgb(train_df: pl.DataFrame):\n    \"\"\"\n    LightGBM Classifier 학습 (원본 구조 유지).\n    \"\"\"\n    global MODEL_LGB, TRAIN_COLS_LGB, TRAIN_MEANS_LGB\n\n    y_train = (train_df[\"market_forward_excess_returns\"].to_numpy() > 0).astype(int)\n    X_train_pd = preprocess_data_lgb(train_df, is_training=True)\n\n    TRAIN_COLS_LGB = list(X_train_pd.columns)\n    TRAIN_MEANS_LGB = X_train_pd.mean()\n    X_train_pd = X_train_pd.fillna(TRAIN_MEANS_LGB)\n\n    regressor = lgb.LGBMClassifier(\n        objective=\"binary\",\n        n_estimators=10000,\n        learning_rate=0.006,\n        max_depth=-1,\n        num_leaves=511,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        n_jobs=4,\n        reg_lambda=3.0,\n        min_child_samples=15,\n        boosting_type=\"gbdt\",\n        is_unbalance=True,\n    )\n\n    MODEL_LGB = regressor\n\n    logger.info(\n        f\"Starting LightGBM model training on {len(X_train_pd)} samples with {len(TRAIN_COLS_LGB)} features...\"\n    )\n    MODEL_LGB.fit(X_train_pd, y_train)\n    logger.info(\"LightGBM model training complete.\")\n\nLGB_CONFIDENCE_MULTIPLIER = 6.0   # 기존 6.0 → 4~8 정도에서 튜닝\nLGB_DEAD_ZONE = 0.015             # 기존 0.015 → 0.01~0.03 정도에서 튜닝\n\n# def convert_prediction_to_allocation(p_positive: float) -> float:\n#     \"\"\"\n#     원본 LightGBM 모델의 allocation 변환 함수.\n#     \"\"\"\n#     CONFIDENCE_MULTIPLIER = 6.0\n#     edge = p_positive - 0.5\n\n#     # 작은 Edge는 트레이드 억제\n#     if abs(edge) < 0.015:\n#         return 1.0\n\n#     scaled_edge = np.tanh(CONFIDENCE_MULTIPLIER * edge)\n#     final_allocation = 1.0 + scaled_edge\n#     final_allocation = np.clip(final_allocation, 0.0, 2.0)\n#     return float(final_allocation)\n\ndef convert_prediction_to_allocation(p_positive: float) -> float:\n    \"\"\"\n    LightGBM의 P(Up) -> 최종 allocation 변환 로직.\n\n    - LGB_DEAD_ZONE: 0.5 근처의 애매한 확률은 트레이드 하지 않음 (allocation=1.0)\n    - LGB_CONFIDENCE_MULTIPLIER: tanh 스케일링 강도 (클수록 0/2 쪽으로 빠르게 포화)\n    \"\"\"\n    edge = p_positive - 0.5  # 0보다 크면 상승 쪽, 작으면 하락 쪽\n\n    # 작은 Edge는 트레이드 억제 (중립 비중 유지)\n    if abs(edge) < LGB_DEAD_ZONE:\n        return 1.0\n\n    # 확신도가 커질수록 1에서 멀어지게 (0 ~ 2 범위)\n    scaled_edge = np.tanh(LGB_CONFIDENCE_MULTIPLIER * edge)\n    final_allocation = 1.0 + scaled_edge   # edge>0이면 1~2, edge<0이면 1~0\n\n    return float(np.clip(final_allocation, 0.0, 2.0))\n\ndef ensure_lgb_model_trained() -> bool:\n    \"\"\"\n    MODEL_LGB가 없으면 train.csv를 읽어 학습.\n    \"\"\"\n    global MODEL_LGB\n    if MODEL_LGB is not None:\n        return True\n\n    train_path = os.path.join(\n        \"/kaggle/input/hull-tactical-market-prediction/\", \"train.csv\"\n    )\n    try:\n        train_df = pl.read_csv(train_path, try_parse_dates=True, infer_schema_length=100000)\n    except Exception as e:\n        logger.error(f\"Could not load train.csv for LGBM: {e}\")\n        return False\n\n    train_model_lgb(train_df)\n    if MODEL_LGB is None:\n        logger.error(\"LightGBM model training failed.\")\n        return False\n\n    return True\n\ndef predict_lgb_component(test: pl.DataFrame, is_mock_run: bool):\n    \"\"\"\n    원본 predict 로직을 그대로 사용하되,\n    (allocation, p_positive)를 반환하도록 분리한 내부 함수.\n    \"\"\"\n    global MODEL_LGB, TRAIN_COLS_LGB, TRAIN_MEANS_LGB\n\n    default_alloc = 1.0\n    default_p = 0.5\n\n    if not ensure_lgb_model_trained():\n        return default_alloc, default_p\n\n    try:\n        X_test_pd = preprocess_data_lgb(test)\n        missing_cols = set(TRAIN_COLS_LGB) - set(X_test_pd.columns)\n        for col in missing_cols:\n            X_test_pd[col] = np.nan\n        X_test_pd = X_test_pd[TRAIN_COLS_LGB]\n\n        if TRAIN_MEANS_LGB is not None:\n            X_test_pd = X_test_pd.fillna(TRAIN_MEANS_LGB)\n        else:\n            X_test_pd = X_test_pd.fillna(X_test_pd.mean())\n    except Exception as e:\n        logger.error(f\"LightGBM feature selection failed in predict: {e}\")\n        return default_alloc, default_p\n\n    try:\n        predicted_probabilities = MODEL_LGB.predict_proba(X_test_pd)\n        p_positive = predicted_probabilities[-1, 1]\n        alloc = convert_prediction_to_allocation(p_positive)\n        return float(alloc), float(p_positive)\n    except Exception as e:\n        logger.error(f\"LightGBM inference failed: {e}\")\n        return default_alloc, default_p\n\n\n# =============================================================================\n# MODEL 2: Simple Strategy + XGBoost ML Fallback (원본 구조 유지, 이름만 변경)\n# =============================================================================\n\nlogger.info(\"Loading training data for XGBoost fallback...\")\ntrain_full_xgb = pl.read_csv(DATA_PATH / \"train.csv\")\n\nTRUE_RETURNS_DICT = {\n    int(row[\"date_id\"]): float(row[\"forward_returns\"])\n    for row in train_full_xgb.select([\"date_id\", \"forward_returns\"]).iter_rows(named=True)\n}\n\nlogger.info(f\"Loaded {len(TRUE_RETURNS_DICT):,} returns for XGBoost model\")\n\nlast_180 = train_full_xgb.tail(180)\nlast_180_returns = last_180[\"forward_returns\"].to_numpy()\n\npositive_returns = last_180_returns[last_180_returns > 0]\nnegative_returns = last_180_returns[last_180_returns <= 0]\n\navg_positive = positive_returns.mean()\navg_negative = negative_returns.mean()\n\nlogger.info(\"Last 180 statistics (XGB helper):\")\nlogger.info(f\"  Positive days: {len(positive_returns)}\")\nlogger.info(f\"  Avg positive: {avg_positive:.6f}\")\nlogger.info(f\"  Negative days: {len(negative_returns)}\")\nlogger.info(f\"  Avg negative: {avg_negative:.6f}\")\n\nALPHA_POSITIVE_XGB = 0.90\n\nlogger.info(f\"Using alpha (XGB) = {ALPHA_POSITIVE_XGB}\")\n\nlogger.info(\"Training XGBoost fallback model...\")\ntrain_recent_xgb = train_full_xgb.tail(800)\n\nSCALE_Y_XGB = 400.0\n\n# FEATURE_COLS_XGB = []\n# for col in train_recent_xgb.columns:\n#     if col.startswith((\"M\", \"E\", \"I\", \"P\", \"V\", \"S\")):\n#         if train_recent_xgb[col].is_null().mean() < 0.5:\n#             FEATURE_COLS_XGB.append(col)\n\n# X_ml = train_recent_xgb.select(FEATURE_COLS_XGB).fill_null(0).to_pandas()\n# y_ml = train_recent_xgb[\"market_forward_excess_returns\"].fill_null(0).to_pandas()\n\n# SCALER_XGB = StandardScaler()\n# X_ml_scaled = SCALER_XGB.fit_transform(X_ml)\n\nNUMERIC_BASE_COLS = [\n    \"M1\", \"M2\", \"E1\", \"E2\", \"V1\", \"V2\",\n    \"S1\", \"S2\", \"T1\", \"T2\", \"P1\", \"P2\",\n    \"D1\", \"D2\"\n]\ntrain_recent_xgb = train_recent_xgb.with_columns(\n    [\n        pl.col(c).cast(pl.Float64)\n        for c in NUMERIC_BASE_COLS\n        if c in train_recent_xgb.columns\n    ]\n)\n\n# 이제 LGBM용 전처리를 그대로 재사용\nX_ml_pd = preprocess_data_lgb(train_recent_xgb)\nFEATURE_COLS_XGB = list(X_ml_pd.columns)\n\n# y_ml = train_recent_xgb[\"market_forward_excess_returns\"].fill_null(0).to_pandas()\n# 원래 수익률\ny_ml_raw = train_recent_xgb[\"market_forward_excess_returns\"].fill_null(0).to_pandas()\n# 스케일링된 타깃 (예: × 400)\ny_ml = y_ml_raw * SCALE_Y_XGB\n\nSCALER_XGB = StandardScaler()\nX_ml_scaled = SCALER_XGB.fit_transform(X_ml_pd)\n\nXGB_MODEL = xgb.XGBRegressor(\n    n_estimators=1600,\n    learning_rate=0.015,\n    max_depth=5,\n    min_child_weight=1,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    verbosity=0,\n)\nXGB_MODEL.fit(X_ml_scaled, y_ml)\n\nlogger.info(\"XGBoost ML fallback model ready.\")\n\nPREDICTION_COUNT_XGB = 0\n\ndef predict_xgb_component(test: pl.DataFrame) -> float:\n    \"\"\"\n    원본 XGBoost 모델의 predict 로직.\n    (true_return 기반 simple 전략 부분은 주석 처리된 상태 그대로 유지.)\n    \"\"\"\n    global PREDICTION_COUNT_XGB\n\n    date_id = int(test.select(\"date_id\").to_series().item())\n    true_return = TRUE_RETURNS_DICT.get(date_id)\n\n    # 원본 코드의 PUBLIC TEST 분기 (현재 주석 상태라 그대로 유지)\n    # if true_return is not None:\n    #     if true_return > 0:\n    #         position = ALPHA_POSITIVE_XGB\n    #     else:\n#         position = 0.0\n# \n#         PREDICTION_COUNT_XGB += 1\n#         return float(position)\n\n    # PRIVATE TEST / 일반 모드: ML fallback\n    try:\n        # X_test = test.select(FEATURE_COLS_XGB).fill_null(0).to_pandas()\n        # X_test_scaled = SCALER_XGB.transform(X_test)\n        # ml_pred = XGB_MODEL.predict(X_test_scaled)[0]\n        # position = np.clip(ml_pred * 400, 0, 2)\n        X_test = test.select(FEATURE_COLS_XGB).fill_null(0).to_pandas()\n        X_test_scaled = SCALER_XGB.transform(X_test)\n\n        # 1) 스케일링된 타깃에 대해 예측된 값\n        ml_pred_scaled = XGB_MODEL.predict(X_test_scaled)[0]\n\n        # 2) 원래 수익률 스케일로 복원\n        raw_ret = ml_pred_scaled / SCALE_Y_XGB\n\n        # 3) 공통 post-process 로 allocation 계산\n        #    post_process_signal은 배열을 받으므로 [raw_ret] 형태로 넘긴 뒤 [0]만 가져옴\n        alloc_arr = post_process_signal([raw_ret])\n        position = float(np.asarray(alloc_arr).ravel()[0])\n    except Exception as e:\n        logger.error(f\"XGBoost inference failed: {e}\")\n        position = 0.0\n\n    PREDICTION_COUNT_XGB += 1\n    return float(position)\n\n\n# =============================================================================\n# MODEL 3: GradientBoostingRegressor + post_process_signal (원본 구조 유지, 이름만 변경)\n# =============================================================================\n\nALPHA_FOR_SCORER = 0.600132\nTAU_ABS_FOR_SCORER = 9.43717e-05\nMIN_INVESTMENT, MAX_INVESTMENT = 0.0, 2.0\nTRADING_DAYS = 252\n\ndef post_process_signal(\n    y_pred,\n    *,\n    tau: float = TAU_ABS_FOR_SCORER,\n    alpha: float = ALPHA_FOR_SCORER,\n    min_investment: float = MIN_INVESTMENT,\n    max_investment: float = MAX_INVESTMENT,\n):\n    sig = np.asarray(y_pred, dtype=float).ravel()\n    pos = np.where(sig > tau, alpha, 0.0)\n    return np.clip(pos, min_investment, max_investment)\n\nPATH_GB = \"/kaggle/input/hull-tactical-market-prediction/\"\ntrain_gb = pd.read_csv(f\"{PATH_GB}train.csv\")\n\nTARGET_GB = \"forward_returns\"\nif TARGET_GB not in train_gb.columns:\n    raise ValueError(\n        f\"Expected target column '{TARGET_GB}' in train.csv; found: {list(train_gb.columns)}\"\n    )\n\nDROP_IF_EXISTS_GB = [\n    \"row_id\",\n    \"id\",\n    \"risk_free_rate\",\n    \"market_forward_excess_returns\",\n]\nuse_cols_gb = [c for c in train_gb.columns if c not in DROP_IF_EXISTS_GB]\ntrain_gb = train_gb[use_cols_gb]\n\ndef preprocess_gb(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    high_null_cols = [c for c in df.columns if df[c].isnull().mean() > 0.5]\n    df = df.drop(columns=high_null_cols, errors=\"ignore\")\n\n    for col in df.columns:\n        if df[col].dtype in [\"float64\", \"int64\"]:\n            df[col] = df[col].fillna(df[col].median())\n        else:\n            if len(df[col].mode()) > 0:\n                df[col] = df[col].fillna(df[col].mode()[0])\n\n    return df\n\ntrain_gb = preprocess_gb(train_gb)\n\nX_gb = train_gb.drop(columns=[TARGET_GB])\ny_gb = train_gb[TARGET_GB]\n\nprint(f\"Features shape (GB): {X_gb.shape}\")\nprint(f\"Target shape (GB): {y_gb.shape}\")\n\nFEATURE_COLS_GB = X_gb.columns.tolist()\n\nx_train_gb, x_val_gb, y_train_gb, y_val_gb = train_test_split(\n    X_gb, y_gb, random_state=123, test_size=0.2\n)\n\nSCALER_GB = StandardScaler()\nX_train_scaled_gb = SCALER_GB.fit_transform(x_train_gb)\nX_val_scaled_gb = SCALER_GB.transform(x_val_gb)\n\nGB_MODEL = GradientBoostingRegressor(\n    n_estimators=1600,\n    learning_rate=0.04,\n    max_depth=12,\n    subsample=0.7,\n    max_features=0.7,\n    random_state=123,\n    verbose=0,\n)\n\nprint(\"training GB model...\")\nGB_MODEL.fit(X_train_scaled_gb, y_train_gb)\nprint(\"GB training completed!\")\n\ny_pred_train_gb = GB_MODEL.predict(X_train_scaled_gb)\ny_pred_val_gb = GB_MODEL.predict(X_val_scaled_gb)\n\ntrain_rmse_gb = np.sqrt(mean_squared_error(y_train_gb, y_pred_train_gb))\nval_rmse_gb = np.sqrt(mean_squared_error(y_val_gb, y_pred_val_gb))\n\nprint(f\"GB training RMSE: {train_rmse_gb:.4f}\")\nprint(f\"GB validation RMSE: {val_rmse_gb:.4f}\")\nprint(f\"GB overfitting ratio: {val_rmse_gb/train_rmse_gb:.4f}\")\n\ndef predict_gb_component(test: pl.DataFrame) -> float:\n    \"\"\"\n    원본 GradientBoostingRegressor 기반 predict 로직.\n    \"\"\"\n    if not isinstance(test, pl.DataFrame):\n        raise TypeError(\"predict(test): expected a Polars DataFrame input\")\n\n    if test.height != 1:\n        raise ValueError(\n            f\"predict(test): expected a single-row Polars DataFrame, got {test.height} rows\"\n        )\n\n    drop_cols = [c for c in DROP_IF_EXISTS_GB if c in test.columns]\n    test_pl = test.drop(drop_cols) if drop_cols else test\n\n    if TARGET_GB in test_pl.columns:\n        test_pl = test_pl.drop(TARGET_GB)\n\n    test_pd = test_pl.to_pandas()\n    test_pd = preprocess_gb(test_pd)\n    test_pd = test_pd.reindex(columns=FEATURE_COLS_GB, fill_value=0)\n    test_scaled = SCALER_GB.transform(test_pd)\n\n    raw = GB_MODEL.predict(test_scaled)\n    pos = post_process_signal(raw)\n    return float(np.asarray(pos).ravel()[0])\n\n\n# =============================================================================\n# Visualization Helper (원본 LightGBM mock 시각화 유지)\n# =============================================================================\n\ndef plot_results(results_df: pd.DataFrame):\n    \"\"\"Generates a plot of the predicted returns and allocations.\"\"\"\n    fig, ax1 = plt.subplots(figsize=(10, 5))\n\n    color = \"tab:blue\"\n    ax1.set_xlabel(\"Simulated Day\")\n    ax1.set_ylabel(\"Predicted Probability (P > 0)\", color=color)\n    ax1.plot(\n        results_df[\"day\"],\n        results_df[\"predicted_return\"],\n        color=color,\n        label=\"Predicted P(Up)\",\n        alpha=0.6,\n    )\n    ax1.tick_params(axis=\"y\", labelcolor=color)\n    ax1.grid(True, linestyle=\"--\", alpha=0.5)\n    ax1.axhline(0.5, color=\"orange\", linestyle=\"--\", label=\"Neutral P(0.5)\")\n\n    ax2 = ax1.twinx()\n    color = \"tab:red\"\n    ax2.set_ylabel(\"Final Allocation (0.0 to 2.0)\", color=color)\n    ax2.plot(\n        results_df[\"day\"],\n        results_df[\"allocation\"],\n        color=color,\n        label=\"Final Allocation\",\n        linewidth=2,\n    )\n    ax2.tick_params(axis=\"y\", labelcolor=color)\n    ax2.axhline(1.0, color=\"gray\", linestyle=\":\", label=\"Neutral (1.0)\")\n\n    fig.suptitle(\n        \"ENSEMBLED Mock Prediction and Allocation over 50 Days (Aggressive Params)\"\n    )\n    fig.tight_layout()\n    plt.show()\n\ndef run_mock_test_and_visualize():\n    \"\"\"\n    원본 LightGBM mock 테스트 로직 유지.\n    이제는 앙상블된 predict()를 사용.\n    \"\"\"\n    MOCK_FEATURES = [\n        \"D1\",\n        \"D2\",\n        \"E1\",\n        \"E2\",\n        \"V1\",\n        \"V2\",\n        \"S1\",\n        \"S2\",\n        \"M1\",\n        \"M2\",\n        \"T1\",\n        \"T2\",\n        \"P1\",\n        \"P2\",\n    ]\n\n    results = []\n\n    mock_data_init = {col: [np.random.rand()] for col in MOCK_FEATURES}\n    mock_data_init[\"market_forward_excess_returns\"] = [np.random.uniform(-0.01, 0.01)]\n    mock_data_init[\"date_id\"] = [1000]\n    mock_test_df_init = pl.DataFrame(mock_data_init)\n\n    logger.info(\n        \"Starting initial predict call (triggers LightGBM training simulation)...\"\n    )\n    _ = predict(mock_test_df_init)\n    logger.info(\"Models are now trained and ready for inference.\")\n\n    NUM_SIMULATION_DAYS = 50\n    for day in range(NUM_SIMULATION_DAYS):\n        mock_day_data = {\n            c: [np.random.uniform(0.1, 0.9) if c == \"M1\" else np.random.rand()]\n            for c in MOCK_FEATURES\n        }\n        mock_day_data[\"market_forward_excess_returns\"] = [\n            np.random.uniform(-0.01, 0.01)\n        ]\n        mock_day_data[\"date_id\"] = [1001 + day]\n        mock_test_df_day = pl.DataFrame(mock_day_data)\n\n        allocation, p_positive = predict(mock_test_df_day)\n\n        results.append(\n            {\n                \"day\": day + 1,\n                \"predicted_return\": p_positive,\n                \"allocation\": allocation,\n            }\n        )\n\n    results_df = pd.DataFrame(results)\n\n    logger.info(\"\\n--- MOCK TEST SIMULATION SUMMARY (50 Days) ---\")\n    logger.info(\n        f\"Mean Predicted Probability (P > 0): {results_df['predicted_return'].mean():.6f}\"\n    )\n    logger.info(\n        f\"Mean Final Allocation: {results_df['allocation'].mean():.4f}\"\n    )\n    logger.info(\n        f\"Min/Max Allocation: {results_df['allocation'].min():.4f} / {results_df['allocation'].max():.4f}\"\n    )\n    logger.info(\"-------------------------------------------------\")\n\n    plot_results(results_df)\n\n\n# =============================================================================\n# FINAL PREDICT: 세 모델 앙상블 (LightGBM + XGB + GBM)\n# =============================================================================\n\ndef predict(test: pl.DataFrame):\n    \"\"\"\n    Kaggle에서 호출되는 최종 predict 함수.\n\n    - Kaggle 재실행 환경(KAGGLE_IS_COMPETITION_RERUN=True)에서는 float (allocation) 반환\n    - 로컬/mock 환경에서는 (allocation, p_positive_from_LGB) 튜플 반환\n      (원래 LightGBM 코드의 인터페이스를 그대로 유지)\n    \"\"\"\n    is_mock_run = not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n    default_return = (1.0, 0.5) if is_mock_run else 1.0\n\n    # 1) LightGBM 기반 확률 + allocation\n    alloc_lgb, p_lgb = predict_lgb_component(test, is_mock_run=is_mock_run)\n\n    # 2) XGBoost Fallback 모델 allocation\n    try:\n        alloc_xgb = predict_xgb_component(test)\n    except Exception as e:\n        logger.error(f\"XGBoost component failed: {e}\")\n        alloc_xgb = alloc_lgb\n\n    # 3) GradientBoostingRegressor 기반 allocation\n    try:\n        alloc_gb = predict_gb_component(test)\n    except Exception as e:\n        logger.error(f\"GBM component failed: {e}\")\n        alloc_gb = alloc_lgb\n\n    # 4) 세 개 모델의 앙상블\n    final_alloc = ensemble_allocation(alloc_lgb, alloc_xgb, alloc_gb)\n\n    if is_mock_run:\n        # 원래 LightGBM 코드처럼 (allocation, p_positive) 형식을 유지\n        return (final_alloc, p_lgb)\n\n    return final_alloc\n\n\n# =============================================================================\n# MAIN: Inference Server + submission.parquet 생성 방식은 원본과 동일\n# =============================================================================\n\ninference_server_instance = DefaultInferenceServer(predict)\n\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    inference_server_instance.serve()\nelse:\n    logger.info(\"Running local gateway for testing.\")\n    try:\n        local_input_path = os.path.join(\n            os.getcwd(), \"kaggle_input/hull-tactical-market-prediction/\"\n        )\n        inference_server_instance.run_local_gateway((local_input_path,))\n    except Exception as e:\n        logger.error(\n            f\"Local gateway simulation failed. This is expected outside Kaggle: {e}\"\n        )\n\n        # 원본 코드와 동일하게 mock test + 시각화 + dummy submission 생성\n        run_mock_test_and_visualize()\n\n        logger.info(\"Generating dummy submission.parquet for Kaggle system check.\")\n        dummy_submission = pl.DataFrame(\n            {\"date_id\": [999], \"allocation\": [1.0]}\n        )\n        dummy_submission.write_parquet(\"submission.parquet\")\n        logger.info(\"submission.parquet created successfully.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T10:34:13.929363Z","iopub.execute_input":"2025-11-19T10:34:13.930637Z","iopub.status.idle":"2025-11-19T10:34:26.587333Z","shell.execute_reply.started":"2025-11-19T10:34:13.930592Z","shell.execute_reply":"2025-11-19T10:34:26.586004Z"}},"outputs":[],"execution_count":null}]}